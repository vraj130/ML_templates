{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2155fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here we import all the necessay libraries\n",
    "#to make sure we dont miss out on any function we also import some extra \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import missingno as mno\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.preprocessing import LabelBinarizer, Normalizer\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_log_error, mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBRegressor\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb41abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## First read the train data file and see its head\n",
    "train = pd.read_csv(\"train_file_name.csv\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80313102",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we check shape i.e (number of trainign samples and the no. of features )\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0528e317",
   "metadata": {},
   "outputs": [],
   "source": [
    "##We check the columns in the data\n",
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079a10d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "##WE see some type and the no. of non numm values in the data \n",
    "train.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b99678f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next is we check the number of null values for each feature\n",
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f04e349",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if the above shows null for numeric we if first replace it with mean or median values\n",
    "train.loc[train['column_name'].isna(), 'column_name'] = train['column_name'].median()\n",
    "\n",
    "#If if null exist  for categorical values we should just drop it as it cannot be imputed\n",
    "train = train.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d594498",
   "metadata": {},
   "outputs": [],
   "source": [
    "##After handling the null we check if all the nul lvalues are handled in the data\n",
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a0db9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we check if a certain column does not contribute to the model and drop it\n",
    "train.drop[columns=['column_name'], axis=1, inplace=True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad932bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We replace the binary categorical colums with0 and 1\n",
    "train['column_name'] = train['column_name'].map({'yes':1, 'no':0})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d402004",
   "metadata": {},
   "source": [
    "We now continue with the Data Visualizatinon part\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4c3b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we make a copy for the data for the visualizations of the data and encode the categorical data\n",
    "# for our visualization purpose.\n",
    "train_copy = train.copy()\n",
    "\n",
    "colums_enc = ['column1', 'column2', 'column3']\n",
    "\n",
    "for col in colums_enc:\n",
    "    for i, item in enumerate(train[col].unique()):\n",
    "        train_copy[col] = train_copy[col].replace(item, i)\n",
    "\n",
    "## The above command will give a unique number to the categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5b2744",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bd897d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3fcfa5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c610effd",
   "metadata": {},
   "source": [
    "Data Encodeing and Normaliztion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284aa38e",
   "metadata": {},
   "source": [
    "1) Encodeing for Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ccffa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we make copy to avoid data conflict when running the jupytertabs in non sequential order\n",
    "train_data = train.copy()\n",
    "\n",
    "## We select the colums with categorical data and we encode them\n",
    "colums_enc = ['column1', 'column2', 'column3']\n",
    "train_data = pd.get_dummies(train_data, columns=colums_enc)\n",
    "\n",
    "train_data = train_data.drop(columns=['column1', 'column2', 'column3'], axis=1)\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84133f07",
   "metadata": {},
   "source": [
    "2. Normalization for Numeric Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac216632",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the Scaler function  library\n",
    "num_col = ['column1', 'column2', 'column3']\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_data[num_col])\n",
    "train_data[num_col] = scaler.transform(train_data[num_col])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddceb395",
   "metadata": {},
   "source": [
    "Here we begin with the Model Training part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf10d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before we do the traiin  test split we remove the target column from the data\n",
    "Y = train_data['target_col']\n",
    "X = train_data.drop(columns=['target_col'], axis=1)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9570fe29",
   "metadata": {},
   "source": [
    "hERE WE SELECT MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659b212f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##If the problem is regression we use the linear regression model\n",
    "model =  RandomForestRegressor()\n",
    "model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de30925",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if the problem is classification we use the logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "## OR in classification itself we use the XGBClassifier model\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, Y_train)\n",
    "accuracy_score(Y_test, model.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb36d34f",
   "metadata": {},
   "source": [
    "Feature Importance from the Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a913d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Neel Aiya data nakh\n",
    "#Feature importances FROM THE RANDOM FOREST "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d01b23a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7728490c",
   "metadata": {},
   "source": [
    "MODEL Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4e80a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"test_file_name.csv\")\n",
    "test_data = test.copy()\n",
    "test_data_withid = test.copy()\n",
    "\n",
    "test_data.drop[columns=['id'], axis=1, inplace=True]\n",
    "test_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001a8532",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if the above shows null for numeric we if first replace it with mean or median values\n",
    "test_data.loc[train['column_name'].isna(), 'column_name'] = test_data['column_name'].median()\n",
    "\n",
    "#If if null exist  for categorical values we should just drop it as it cannot be imputed\n",
    "tetest_datast = test_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaede626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all the null values are handled in the data\n",
    "test_data.isnull.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffa5421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We drop the columns that are not needed\n",
    "test_data.drop[columns=['column_name'], axis=1, inplace=True]\n",
    "\n",
    "# We replace the binary categorical colums with0 and 1\n",
    "tetest_data['column_name'] = test_data['column_name'].map({'yes':1, 'no':0})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0bd92f",
   "metadata": {},
   "source": [
    "Again Encode the and Normalize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0125af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## We select the colums with categorical data and we encode them\n",
    "colums_enc = ['column1', 'column2', 'column3']\n",
    "test_data = pd.get_dummies(test_data, columns=colums_enc)\n",
    "\n",
    "test_data = test_data.drop(columns=['column1', 'column2', 'column3'], axis=1)\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63aed270",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the Scaler function  library\n",
    "num_col = ['column1', 'column2', 'column3']\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(test_data[num_col])\n",
    "test_data[num_col] = scaler.transform(test_data[num_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a988d92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = model.predict(test_data)\n",
    "\n",
    "submission_out = pd.DataFrame('id':test_data_withid['id'], 'target_col':submission)\n",
    "\n",
    "submission_out.to_csv('submission_file_name.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
